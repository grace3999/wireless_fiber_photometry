{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grace3999/wireless_fiber_photometry/blob/master/FP_PDsingle_TTL_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hbDyKwNzPeou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b17e9b-bbf0-4451-9bc8-cfccfe3abec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tn8zJeO6PcrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea171bc9-5240-4bda-8423-de2f311869f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a3963c680804>:25: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        }
      ],
      "source": [
        "#getting and working with data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import datetime as dt\n",
        "import string\n",
        "from numpy import trapz\n",
        "\n",
        "from scipy import ndimage\n",
        "from scipy import signal as ss\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "#visualizing results\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_context('poster', rc={'font.size':35,\n",
        "                              'axes.titlesize':50,\n",
        "                              'axes.labelsize':35})\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 15000)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi6aFDmTPcrP"
      },
      "source": [
        "#### Get paths - separate folders for each day/animal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "702NeP3-PcrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc748893-8195-4e03-dde9-6e4ca7dfa9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1610_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1609_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1597_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1600_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1608_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1607_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1596_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1605_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_75.1_1598_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1610_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1608_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1609_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1607_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1600_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1598_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1597_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1596_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_50.1_1605_211130',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1608_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1609_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1610_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1605_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1607_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1600_211201',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1597_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1598_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1596_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_25.1_1598_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1596_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1597_211129',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1609_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1610_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1608_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1605_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1607_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1600_211126',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1600_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1608_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1607_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1609_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1610_211202',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1598_211203',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1596_211203',\n",
              " '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_0.1_1597_211203']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "outer_path = '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced'\n",
        "\n",
        "outer_file_list = os.listdir(outer_path)\n",
        "\n",
        "outer_path_list = []\n",
        "\n",
        "for name in outer_file_list:\n",
        "    int_path = outer_path + '/' + name\n",
        "    outer_path_list.append(int_path)\n",
        "\n",
        "print(len(outer_path_list))\n",
        "outer_path_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game plan:\n",
        "\n",
        "- read in fiber photometry and TTL txt files and combine into dataframe\n",
        "- convert TTL logic into event info\n",
        "- create list of trial times  based on event info\n",
        "- use trial times to make 1 trace per trial containing:\n",
        "  - 10 seconds baseline (previous ITI)\n",
        "  - 5 seconds lever extend -> lever press\n",
        "  - 5 seconds lever press -> head entry for pellet\n",
        "  - 10 seconds postline (start of ITI)"
      ],
      "metadata": {
        "id": "1SzJJTnI1svy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ITI ends                        10 seconds prior to HE trial start\n",
        "# HE/levers extend                times_lever_extend = []\n",
        "# lever press/reward/ITI start    times_ITI_start = []\n",
        "# reward HE                       times_HE_pellet = []\n",
        "# ITI ends                        20 seconds after reward HE\n",
        "\n",
        "# ITI start -> levers extend (head entry) -> lever press (reward) -> ITI start\n",
        "\n",
        "#make_FP_TTL_list(path)                                                                return FP_TTL_list\n",
        "#make_df_from_file_list(FP_TTL_list)                                                   return TTL_dataframe\n",
        "#make_event_from_TTL(TTL_dataframe)                                                    return TTL_dataframe_session\n",
        "#get_trial_times(TTL_dataframe_session)                                                return times_lever_extend, times_lever_press, times_pellet, times_HE_pellet\n",
        "#make_times_df(times_lever_extend, times_lever_press, times_pellet, times_HE_pellet)   return times_df\n",
        "#make_trials_df(times_df, TTL_dataframe_session)                                       return trials_df"
      ],
      "metadata": {
        "id": "6kfu1EqJwmgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzz3ZxUTxtw"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_FP_TTL_list(path):\n",
        "  TTL_list = []\n",
        "  FP_list = []\n",
        "\n",
        "  inner_file_list = os.listdir(outer_path)\n",
        "    \n",
        "  for inner_file in inner_file_list:\n",
        "        \n",
        "      if inner_file.split('.')[-1]=='TXT':\n",
        "          int_path = outer_path + '/' + inner_file\n",
        "          TTL_list.append(int_path)\n",
        "      else:\n",
        "          int_path = outer_path + '/' + inner_file\n",
        "          FP_list.append(int_path)\n",
        "            \n",
        "  FP_TTL_list = list(zip(sorted(FP_list), sorted(TTL_list)))\n",
        "\n",
        "  return FP_TTL_list"
      ],
      "metadata": {
        "id": "AZwFarPnHUQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_df_from_file_list(FP_TTL_list):\n",
        "\n",
        "  TTL_dataframe = pd.DataFrame(columns=['FP_signal', 'TTL'])\n",
        "  \n",
        "  i=0\n",
        "\n",
        "  for file_pair in FP_TTL_list:\n",
        "\n",
        "    #get meta data from file path\n",
        "    task = file_pair[0].split('/')[-2].split('_')[0]\n",
        "    session = file_pair[0].split('/')[-2].split('_')[1]\n",
        "    animal = file_pair[0].split('/')[-2].split('_')[2]\n",
        "    date = file_pair[0].split('/')[-2].split('_')[3]\n",
        "\n",
        "    #get FP and TTL data and convert to list and put in dataframe\n",
        "    data_int_file = pd.DataFrame(columns=['FP_signal', 'TTL'])\n",
        "    \n",
        "    #FP data\n",
        "    data_FP = pd.melt(pd.read_table(file_pair[0], header=None).T)\n",
        "    #TTL data\n",
        "    data_TTL = pd.melt(pd.read_table(file_pair[1], header=None).T)\n",
        "\n",
        "    data_int_file['FP_signal'] = data_FP['value']\n",
        "    data_int_file['TTL'] = data_TTL['value']\n",
        "    data_int_file['file'] = i\n",
        "\n",
        "    TTL_dataframe = pd.concat([TTL_dataframe, data_int_file], ignore_index=True)\n",
        "    \n",
        "    TTL_dataframe['date'] = date\n",
        "    TTL_dataframe['task'] = task\n",
        "    TTL_dataframe['session'] = session\n",
        "    TTL_dataframe['animal'] = animal\n",
        "      \n",
        "    i+=1\n",
        "\n",
        "  return TTL_dataframe"
      ],
      "metadata": {
        "id": "QT7HGq7uH76r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_event_from_TTL(TTL_dataframe):\n",
        "  \n",
        "  TTL_dataframe = TTL_dataframe.reset_index().sort_values('index')\n",
        "  TTL_dataframe['binary'] = [format(x, \"08b\") for x in TTL_dataframe['TTL']]\n",
        "\n",
        "  TTL_prior = list(TTL_dataframe['binary'][0:-1])\n",
        "  TTL_prior.insert(0,np.nan)\n",
        "  TTL_dataframe['binary_prior'] = TTL_prior\n",
        "\n",
        "  TTL_dataframe['binary_start'] = TTL_dataframe.apply(lambda x : 0 if x['binary'] == x['binary_prior'] else x['binary'], axis=1)\n",
        "\n",
        "  #reset index to get time column\n",
        "\n",
        "  TTL_dataframe['head_entry'] = [x[7] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['left_extend'] = [x[6] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['right_extend'] = [x[5] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['left_press'] = [x[4] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['right_press'] = [x[3] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['pellet'] = [x[2] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['session_start'] = [x[1] for x in TTL_dataframe['binary']]\n",
        "  TTL_dataframe['ITI_start'] = [x[0] for x in TTL_dataframe['binary']]\n",
        "\n",
        "  #remove start of data \n",
        "  start = TTL_dataframe[(TTL_dataframe['binary_start']!=0)&(TTL_dataframe['session_start']=='0')]['index'].values[0]\n",
        "  TTL_dataframe_session = TTL_dataframe[(TTL_dataframe['index']>=start)]\n",
        "  TTL_dataframe_session['session_time'] = np.arange(0, TTL_dataframe_session.shape[0])\n",
        "\n",
        "  return TTL_dataframe_session"
      ],
      "metadata": {
        "id": "CEsz3rI0Syt8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trial_times(TTL_dataframe_session):\n",
        "\n",
        "  times_lever_extend = []\n",
        "  times_lever_press = []\n",
        "  times_pellet = []\n",
        "  times_HE_pellet = []\n",
        "  left_right_extend = []\n",
        "\n",
        "  times = TTL_dataframe_session[(TTL_dataframe_session['binary_start']!=0)&((TTL_dataframe_session['right_extend']=='0')|(TTL_dataframe_session['left_extend']=='0'))]['session_time'].values\n",
        "  prev_time = -5000\n",
        "  for time in times:\n",
        "  \n",
        "    if time - prev_time < 5000:\n",
        "      print('skip')\n",
        "      continue\n",
        "\n",
        "    if time - prev_time > 5000:\n",
        "      times_lever_extend.append((time))\n",
        "\n",
        "      #trial type\n",
        "      if TTL_dataframe_session[(TTL_dataframe_session['session_time']==time)]['left_extend'].values[0]=='0':\n",
        "        left_right_extend.append('left')\n",
        "      else:\n",
        "        left_right_extend.append('right')\n",
        "\n",
        "      #find first lever press (ITI start) after lever extend\n",
        "      df_after_extend = TTL_dataframe_session[TTL_dataframe_session['session_time']>=time]\n",
        "      try:\n",
        "        extend_press_time = df_after_extend[(df_after_extend['binary_start']!=0)&((df_after_extend['right_press']=='0')|(df_after_extend['left_press']=='0'))]['session_time'].values[0]\n",
        "        #must press in 10 seconds\n",
        "        if extend_press_time < (time+10000):\n",
        "          times_lever_press.append(extend_press_time)\n",
        "\n",
        "          #find first pellet after lever press (ITI start)\n",
        "          df_after_press = df_after_extend[df_after_extend['session_time']>=extend_press_time]\n",
        "          try:\n",
        "            #should happen within seconds\n",
        "            press_pellet_time = df_after_press[(df_after_press['binary_start']!=0)&(df_after_press['pellet']=='0')]['session_time'].values[0]\n",
        "            if press_pellet_time < (extend_press_time+5000):\n",
        "              times_pellet.append(press_pellet_time)\n",
        "\n",
        "              #find first HE after lever press (ITI start) \n",
        "              df_after_pellet = df_after_press[df_after_press['session_time']>=press_pellet_time]\n",
        "              try:\n",
        "              #should happen within seconds\n",
        "                pellet_HE_time = df_after_pellet[(df_after_pellet['binary_start']!=0)&(df_after_pellet['head_entry']=='0')]['session_time'].values[0]\n",
        "                if pellet_HE_time < (press_pellet_time+5000):\n",
        "                  times_HE_pellet.append(pellet_HE_time)\n",
        "\n",
        "                else:\n",
        "                  times_HE_pellet.append(np.nan)\n",
        "              except: \n",
        "                times_HE_pellet.append(np.nan)\n",
        "\n",
        "            else:\n",
        "              times_pellet.append(np.nan)\n",
        "              times_HE_pellet.append(np.nan)\n",
        "          except: \n",
        "            times_pellet.append(np.nan)\n",
        "            times_HE_pellet.append(np.nan)\n",
        "\n",
        "        else:\n",
        "          times_lever_press.append(np.nan)\n",
        "          times_HE_pellet.append(np.nan)\n",
        "          times_pellet.append(np.nan)\n",
        "      except: \n",
        "        times_lever_press.append(np.nan)\n",
        "        times_HE_pellet.append(np.nan)\n",
        "        times_pellet.append(np.nan)\n",
        "\n",
        "    prev_time = time\n",
        "\n",
        "  return times_lever_extend, times_lever_press, times_pellet, times_HE_pellet, left_right_extend"
      ],
      "metadata": {
        "id": "Ed0dqtD9TkXS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_times_df(times_lever_extend, times_lever_press, times_pellet, times_HE_pellet, left_right_extend):\n",
        "\n",
        "  times_df = pd.DataFrame(data=[times_lever_extend, times_lever_press, times_pellet, times_HE_pellet, left_right_extend]).T\n",
        "  times_df.columns = ['times_lever_extend', 'times_lever_press', 'times_pellet', 'times_HE_pellet', 'left_right_extend']\n",
        "\n",
        "  times_df['times_trial_start'] = times_df['times_lever_extend'] - 20000\n",
        "  times_df.loc[times_df['times_trial_start']<0, 'times_trial_start'] = 0\n",
        "\n",
        "  times_df['times_trial_end'] = times_df['times_lever_press'] + 20000\n",
        "  times_df['trial'] = np.arange(0, times_df.shape[0])\n",
        "\n",
        "  times_df['diff_press'] = times_df['times_lever_press'] - times_df['times_lever_extend']\n",
        "  times_df['diff_pellet'] = times_df['times_pellet'] - times_df['times_lever_press']\n",
        "  times_df['diff_HE'] = times_df['times_HE_pellet'] - times_df['times_pellet']\n",
        "  times_df['diff_trial'] = times_df['times_trial_end'] - times_df['times_trial_start']\n",
        "\n",
        "  times_df.loc[times_df['times_lever_press'].isna(), 'no_press'] = '1'\n",
        "  times_df.loc[times_df['times_pellet'].isna(), 'no_pellet'] = 1\n",
        "\n",
        "  times_df = times_df[['trial', 'left_right_extend', 'no_press', 'no_pellet', \n",
        "           'times_trial_start', 'times_lever_extend', 'times_lever_press', 'times_pellet', 'times_HE_pellet', 'times_trial_end',\n",
        "           'diff_press', 'diff_pellet', 'diff_HE', 'diff_trial']]\n",
        "\n",
        "  return times_df"
      ],
      "metadata": {
        "id": "-nW3QzO2U7OU"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_trace(data):\n",
        "    \n",
        "    data_int = data\n",
        "    \n",
        "    if data_int['FP_signal'].min() > 0:\n",
        "      data_int['disconnect'] = 'no'\n",
        "    else:\n",
        "      data_int['disconnect'] = 'yes'\n",
        "\n",
        "    #apply 5hz lowpass filter \n",
        "    b, a = ss.butter(4, 20, 'low', fs=1000) \n",
        "    FP_signal_5hz = ss.filtfilt(b, a, data_int['FP_signal'])\n",
        "    data_int['FP_signal_5hz'] = FP_signal_5hz\n",
        "\n",
        "    #detrend\n",
        "    FP_signal_detrend = ss.detrend(data_int['FP_signal']) \n",
        "    data_int['FP_signal_detrend'] = FP_signal_detrend\n",
        "\n",
        "    FP_signal_5hz_detrend = ss.detrend(data_int['FP_signal_5hz']) \n",
        "    data_int['FP_signal_5hz_detrend'] = FP_signal_5hz_detrend\n",
        "    FP_signal_detrend_5hz = ss.filtfilt(b, a, data_int['FP_signal_detrend'])\n",
        "    data_int['FP_signal_detrend_5hz'] = FP_signal_detrend_5hz\n",
        "\n",
        "    #median filter to remove disconnects\n",
        "    result = ndimage.median_filter(data_int['FP_signal_detrend_5hz'].values, size=500)\n",
        "    data_int['FP_signal_medfilt'] = result\n",
        "\n",
        "    #z score across trace\n",
        "    numerator = np.subtract(data_int['FP_signal_5hz_detrend'], np.nanmean(data_int['FP_signal_5hz_detrend']))\n",
        "    zscore = np.divide(numerator, np.nanstd(data_int['FP_signal_5hz_detrend']))\n",
        "    data_int['zscore'] = zscore\n",
        "\n",
        "    numerator = np.subtract(data_int['FP_signal_medfilt'], np.nanmean(data_int['FP_signal_medfilt']))\n",
        "    zscore = np.divide(numerator, np.nanstd(data_int['FP_signal_medfilt']))\n",
        "    data_int['zscore_medfilt'] = zscore\n",
        "\n",
        "    return data_int"
      ],
      "metadata": {
        "id": "8aUatd2hZDqO"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_trials_df(times_df, TTL_dataframe_session):\n",
        "\n",
        "  trials_df = pd.DataFrame()\n",
        "\n",
        "  i=0\n",
        "  for time in times_df['times_lever_extend'].unique():\n",
        "\n",
        "    print(time)\n",
        "    \n",
        "    start = times_df['times_trial_start'][i]\n",
        "    extend = times_df['times_lever_extend'][i]\n",
        "    press = times_df['times_lever_press'][i]\n",
        "    HE = times_df['times_HE_pellet'][i]\n",
        "    end = times_df['times_trial_end'][i]\n",
        "\n",
        "    #4 phases of trial to normalize to same time line \n",
        "    # rest -> levers extend -> lever press (reward) -> HE for reward -> rest \n",
        "    df_start_extend = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=start) & (TTL_dataframe_session['session_time']<extend)].sort_values('session_time')\n",
        "    df_extend_press = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=extend) & (TTL_dataframe_session['session_time']<press)].sort_values('session_time')\n",
        "    df_press_HE = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=press) & (TTL_dataframe_session['session_time']<HE)].sort_values('session_time')\n",
        "    df_HE_end = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=HE) & (TTL_dataframe_session['session_time']<end)].sort_values('session_time')\n",
        "\n",
        "    #each trial will have a lever extension\n",
        "    df_start_extend_resample = ss.resample(x=df_start_extend['FP_signal'].values, num=20000)\n",
        "\n",
        "    #mouse doesn't always press (missed trials with time out)\n",
        "    if press > 0:\n",
        "      df_extend_press_resample = ss.resample(x=df_extend_press['FP_signal'].values, num=3000)\n",
        "      #even if mouse presses, some trials there is no pellet (so no head entry for pellet)\n",
        "      if HE > 0:\n",
        "        df_press_HE_resample = ss.resample(x=df_press_HE['FP_signal'].values, num=1000)\n",
        "        df_HE_end_resample = ss.resample(x=df_HE_end['FP_signal'].values, num=20000)\n",
        "      else:\n",
        "        df_press_noHE = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=press) & (TTL_dataframe_session['session_time']<press+1000)].sort_values('session_time')\n",
        "        df_press_HE_resample = ss.resample(x=df_press_noHE['FP_signal'].values, num=1000)\n",
        "        #setup end also \n",
        "        df_noHE_end = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=press+1000) & (TTL_dataframe_session['session_time']<press+21000)].sort_values('session_time')\n",
        "        df_HE_end_resample = ss.resample(x=df_noHE_end['FP_signal'].values, num=20000)\n",
        "    else:\n",
        "      df_extend_nopress = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=extend) & (TTL_dataframe_session['session_time']<extend+3000)].sort_values('session_time')\n",
        "      df_extend_press_resample = ss.resample(x=df_extend_nopress['FP_signal'].values, num=3000)\n",
        "      #mouse also won't have a HE for a pellet also \n",
        "      df_nopress_noHE = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=extend+3000) & (TTL_dataframe_session['session_time']<extend+4000)].sort_values('session_time')\n",
        "      df_press_HE_resample = ss.resample(x=df_nopress_noHE['FP_signal'].values, num=1000)\n",
        "      df_noHE_end = TTL_dataframe_session[(TTL_dataframe_session['session_time']>=extend+4000) & (TTL_dataframe_session['session_time']<extend+24000)].sort_values('session_time')\n",
        "      df_HE_end_resample = ss.resample(x=df_noHE_end['FP_signal'].values, num=20000)\n",
        "\n",
        "    trace_final = np.concatenate((df_start_extend_resample, df_extend_press_resample, df_press_HE_resample, df_HE_end_resample), axis=0)\n",
        "\n",
        "    trials_df_int = pd.DataFrame(columns=['FP_signal'])\n",
        "    trials_df_int['FP_signal'] = trace_final\n",
        "    trials_df_int['index'] = np.arange(0, trials_df_int.shape[0])/1000\n",
        "    trials_df_int['trial'] = i\n",
        "    trials_df_int['no_press'] = times_df[times_df['times_lever_extend']==time]['no_press'].values[0]\n",
        "    trials_df_int['no_pellet'] = times_df[times_df['times_lever_extend']==time]['no_pellet'].values[0]\n",
        "    trials_df_int['left_right_extend'] = times_df[times_df['times_lever_extend']==time]['left_right_extend'].values[0]\n",
        "    trials_df_int['trial_start'] = start\n",
        "\n",
        "    #process trace\n",
        "    trials_df_int = process_trace(trials_df_int)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "    if trials_df.shape[0] == 0:\n",
        "      trials_df = trials_df_int\n",
        "    else:\n",
        "      trials_df = pd.concat([trials_df, trials_df_int], ignore_index=True)\n",
        "\n",
        "  return trials_df"
      ],
      "metadata": {
        "id": "I1cFjrJ6eSFk"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "times_dir_dataframe = pd.DataFrame()\n",
        "trials_dir_dataframe = pd.DataFrame()\n",
        "\n",
        "for outer_path in outer_path_list[-16:-8]:\n",
        "\n",
        "    print(outer_path, '\\n')\n",
        "\n",
        "    #make list of file paths and zip TTL and FP files\n",
        "    print('Making file list...')\n",
        "\n",
        "    if outer_path.split('/') == '.DS_Store':\n",
        "        continue\n",
        "\n",
        "    FP_TTL_list = make_FP_TTL_list(outer_path)\n",
        "\n",
        "    #Combine data from all files and create df\n",
        "    print('Creating data frame from files...')\n",
        "    TTL_dataframe = make_df_from_file_list(FP_TTL_list)\n",
        "\n",
        "    #create TTL_prior and then use to mark start of new TTLs\n",
        "    print('Expanding TTLs...')\n",
        "    TTL_dataframe_session = make_event_from_TTL(TTL_dataframe)\n",
        "\n",
        "    #get trial times for each event of interest\n",
        "    print('Collecting event times...')\n",
        "    times_lever_extend, times_lever_press, times_pellet, times_HE_pellet, left_right_extend = get_trial_times(TTL_dataframe_session)   \n",
        "    times_df = make_times_df(times_lever_extend, times_lever_press, times_pellet, times_HE_pellet, left_right_extend)\n",
        "    print('Trial count: ', times_df['trial'].max())\n",
        "\n",
        "    if times_dir_dataframe.shape[0] == 0:\n",
        "      times_dir_dataframe = times_df\n",
        "    else:\n",
        "      times_dir_dataframe = pd.concat([times_dir_dataframe, times_df], ignore_index=True)\n",
        "\n",
        "    #upsample/downsample to put all trials on same time scale\n",
        "    print('Creating normalized trials...')\n",
        "    trials_df = make_trials_df(times_df, TTL_dataframe_session)  \n",
        "\n",
        "    trials_df['date'] = TTL_dataframe_session['date'].unique()[0]\n",
        "    trials_df['task'] = TTL_dataframe_session['task'].unique()[0]\n",
        "    trials_df['session'] = TTL_dataframe_session['session'].unique()[0]\n",
        "    trials_df['animal'] = TTL_dataframe_session['animal'].unique()[0]\n",
        "    print('Trial count: ', trials_df['trial'].max())\n",
        "\n",
        "    if trials_dir_dataframe.shape[0] == 0:\n",
        "      trials_dir_dataframe = trials_df\n",
        "    else:\n",
        "      trials_dir_dataframe = pd.concat([trials_dir_dataframe, trials_df], ignore_index=True)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "print(times_dir_dataframe.shape)\n",
        "print(trials_dir_dataframe.shape)\n",
        "\n",
        "times_dir_dataframe.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQxI2hCNHuyP",
        "outputId": "8638752d-29a4-4c11-f637-ccde8e205129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1596_211129 \n",
            "\n",
            "Making file list...\n",
            "Creating data frame from files...\n",
            "Expanding TTLs...\n",
            "Collecting event times...\n",
            "Trial count:  22\n",
            "Creating normalized trials...\n",
            "1276\n",
            "47800\n",
            "101300\n",
            "147000\n",
            "192500\n",
            "242800\n",
            "292333\n",
            "305100\n",
            "352252\n",
            "399700\n",
            "449300\n",
            "497800\n",
            "506600\n",
            "560300\n",
            "605600\n",
            "665594\n",
            "742300\n",
            "795900\n",
            "854504\n",
            "904239\n",
            "952300\n",
            "1005700\n",
            "1058700\n",
            "Trial count:  22\n",
            "\n",
            "\n",
            "/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/cohort1/PDT/forced/PDT_100.1_1597_211129 \n",
            "\n",
            "Making file list...\n",
            "Creating data frame from files...\n",
            "Expanding TTLs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times_dir_dataframe.median()"
      ],
      "metadata": {
        "id": "2J_Vva2I7QWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials_dir_dataframe.head()"
      ],
      "metadata": {
        "id": "JZCil3NQ8ZGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_meta = '/content/drive/Shareddrives/Schindler Iterative Translation Lab/data/FP/meta.xlsx'\n",
        "meta = pd.DataFrame(data=pd.read_excel(path_meta))\n",
        "\n",
        "print(meta.shape)\n",
        "meta.tail()"
      ],
      "metadata": {
        "id": "-3R67YzRrRRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials_dir_dataframe['animal'] = [int(x) for x in trials_dir_dataframe['animal']]\n",
        "\n",
        "trials_dir_dataframe = meta.merge(trials_dir_dataframe, on='animal')\n",
        "\n",
        "trials_dir_dataframe.head()"
      ],
      "metadata": {
        "id": "lKtUXZRcrVZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials_dir_dataframe.loc[((trials_dir_dataframe['lever']=='right')&(trials_dir_dataframe['left_right_extend']=='left')), 'prob_lever'] = 'safe'\n",
        "trials_dir_dataframe.loc[((trials_dir_dataframe['lever']=='right')&(trials_dir_dataframe['left_right_extend']=='right')), 'prob_lever'] = 'risky'\n",
        "\n",
        "trials_dir_dataframe.loc[((trials_dir_dataframe['lever']=='left')&(trials_dir_dataframe['left_right_extend']=='right')), 'prob_lever'] = 'safe'\n",
        "trials_dir_dataframe.loc[((trials_dir_dataframe['lever']=='left')&(trials_dir_dataframe['left_right_extend']=='left')), 'prob_lever'] = 'risky'\n",
        "\n",
        "trials_dir_dataframe.head()"
      ],
      "metadata": {
        "id": "n_B-fJiwr4tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "d = trials_dir_dataframe[trials_dir_dataframe['disconnect']=='no']\n",
        "d = d[d['trial']>5]\n",
        "sns.lineplot(x='index', y='zscore_medfilt', data=d, ci=None, n_boot=1, markers=True, hue='prob_lever', size='group')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T8q3fRV07ebC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupby = trials_dir_dataframe[trials_dir_dataframe['prob_lever']=='safe'].groupby(['trial', 'index'])['zscore'].mean().reset_index()\n",
        "groupby = groupby.pivot('trial', 'index', \"zscore\")\n",
        "plt.figure(figsize=(30,9))\n",
        "ax = sns.heatmap(groupby, cmap=\"coolwarm\", vmin=-2, vmax=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XPjY6fgN5hM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDKrphN95hPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lo8ryPoL5hSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsygcaUD5hVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Re5TdNLAlE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t93gKtjNLAp8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtLoggxzLAt2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_tOtGQcQgrI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HuT1IlUO_Ms"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUT2OZ3LO_Pp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeu7GKfWO_Ry"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsgaTTmhO_W8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuL1-7NIO_Zz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}